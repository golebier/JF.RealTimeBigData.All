/*
 ...
*/

package com.github.currencies.batch.computing;

import java.io.IOException;
import java.net.UnknownHostException;
import java.util.ArrayList;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;

import scala.Tuple2;

import com.github.currencies.holder.CurrenciesHolder;
import com.maxmind.geoip2.exception.GeoIp2Exception;

/**
 * Main class to run full currencies count.
 *
 * @author gra
 */
public class Main {
//	private static final Pattern SPACE = Pattern.compile(" ");
//
//	  public static void main(String[] args) {
//	    if (args.length < 2) {
//	      System.err.println("Usage: JavaNetworkWordCount <hostname> <port>");
//	      System.exit(1);
//	    }
//
//	    StreamingExamples.setStreamingLogLevels();
//
//	    // Create the context with a 1 second batch size
//	    SparkConf sparkConf = new SparkConf().setAppName("JavaNetworkWordCount");
//	    JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, Durations.seconds(1));
//
//	    // Create a JavaReceiverInputDStream on target ip:port and count the
//	    // words in input stream of \n delimited text (eg. generated by 'nc')
//	    // Note that no duplication in storage level only for running locally.
//	    // Replication necessary in distributed scenario for fault tolerance.
//	    JavaReceiverInputDStream<String> lines = ssc.socketTextStream(
//	            args[0], Integer.parseInt(args[1]), StorageLevels.MEMORY_AND_DISK_SER);
//	    JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
//	      @Override
//	      public Iterable<String> call(String x) {
//	        return Lists.newArrayList(SPACE.split(x));
//	      }
//	    });
//	    JavaPairDStream<String, Integer> wordCounts = words.mapToPair(
//	      new PairFunction<String, String, Integer>() {
//	        @Override
//	        public Tuple2<String, Integer> call(String s) {
//	          return new Tuple2<String, Integer>(s, 1);
//	        }
//	      }).reduceByKey(new Function2<Integer, Integer, Integer>() {
//	        @Override
//	        public Integer call(Integer i1, Integer i2) {
//	          return i1 + i2;
//	        }
//	      });
//
//	    wordCounts.print();
//	    ssc.start();
//	    ssc.awaitTermination();
//	  }

    public static void main( String[] args ) throws UnknownHostException, IOException, GeoIp2Exception {
	    CHECK_USEGE(args);
	    JavaSparkContext sc = PREPARE_SPARK_CONTEXT();

		// TODO save ES instead
//		computeAndSave(withoutNulls);

		sc.stop();
//		DatabaseReaderWrapper.close();
    }


    // DONE: Message Consumption & Message Processor in one, Spark is ok with that, Test Flume ...

	private static void CHECK_USEGE(String[] args) {
		if (2 != args.length) {
		      System.out.println("Error: expected: <hostname> <port>.");
		      System.exit(-1);
        }
	}

	private static JavaSparkContext PREPARE_SPARK_CONTEXT() {
		SparkConf conf = new SparkConf().setAppName("Batch Analytics");
		conf.set("es.nodes", "elasticsearch-1:9200");
		conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
		conf.set("spark.kryo.registrator", "com.github.currencies.batch.computing.AnalyticsRegistrator");
		JavaSparkContext sc = new JavaSparkContext(conf);
		return sc;
	}
}
